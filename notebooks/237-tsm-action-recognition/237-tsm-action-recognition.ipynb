{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert and Optimize Temporal Shift Module (TSM) with OpenVINOâ„¢\n",
    "\n",
    "The last decade has seen an exponential growth in video data. According to [Statista](https://www.statista.com/statistics/259477/hours-of-video-uploaded-to-youtube-every-minute/), the number of video content hours uploaded every minute on YouTube grew by ~40% between 2014 and 2020. As of June 2022, more than 500 hours of video were uploaded to YouTube every minute. One of the most important tasks that the YouTube platform engages in is the efficient (and automatic) removal of harmful content. \n",
    "\n",
    "This requires the accurate detection and recognition of actions, events, and/or context in the videos. This task is called *video understanding* and is one of the grand challenges in the field of Computer Vision as the temporal order of actions is crucial for accurate classification. For example, an algorithm should be able to differentiate between the action of opening and closing a door.\n",
    "\n",
    "Lin et al. (2019) proposed the [Temporal Shift Module](https://arxiv.org/abs/1811.08383) (TSM) for efficient video understanding. The module allows for joint spatial-temporal modeling by shifting part of the channels along the temporal dimension to exchange information with neighboring frames. The TSM achieves state-of-the-art performance, at the level of 3D convolutional neural networks (CNNs), but at the lower computational cost of 2D CNNs. For more details of the TSM model, see the [paper](https://arxiv.org/abs/1811.08383) and [repository](https://github.com/mit-han-lab/temporal-shift-module).\n",
    "\n",
    "This tutorial provides step-by-step instructions on how to run and optimize the PyTorch TSM model with OpenVINO.\n",
    "\n",
    "The tutorial consists of the following steps:\n",
    "- Prepare and load PyTorch TSM model\n",
    "- Convert PyTorch model to ONNX\n",
    "- Convert ONNX model to OpenVINO IR\n",
    "- Download and prepare dataset\n",
    "- Compare accuracy of PyTorch, ONNX, and OpenVINO IR models\n",
    "- Optimize the OpenVINO IR model using post-training 8-bit integer quantization\n",
    "- Compare accuracy and performance of the FP32 and quantized models.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "As a first step, we will import and install some of the required libraries, download the TSM repository, and set up some constants that will be used throughout the tutorial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from os import PathLike\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"../../notebooks/utils\")\n",
    "from notebook_utils import download_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytube in c:\\code\\internships\\openvino\\openvino_notebooks\\venv\\lib\\site-packages (12.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Code\\Internships\\OpenVINO\\openvino_notebooks\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Install the pytube library for downloading YouTube videos\n",
    "!python -m pip install pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Code\\Internships\\OpenVINO\\openvino_notebooks\\playground\\101-tsm-quantize\\temporal-shift-module\n"
     ]
    }
   ],
   "source": [
    "# Clone TSM repo\n",
    "if not Path('temporal-shift-module').exists():\n",
    "    !git clone https://github.com/mit-han-lab/temporal-shift-module\n",
    "%cd temporal-shift-module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory settings\n",
    "MODEL_DIR = Path(\"../model/\")\n",
    "DATA_ROOT_DIR = Path(\"../data/\")\n",
    "DATA_DIR = DATA_ROOT_DIR / 'kinetics400'\n",
    "IMAGES_DIR = DATA_DIR / 'images'\n",
    "\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "DATA_ROOT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Paths where PyTorch, ONNX, and OpenVINO IR models will be stored.\n",
    "weights_filename = 'TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e50.pth'\n",
    "weights_path = Path(MODEL_DIR) / weights_filename\n",
    "onnx_path = weights_path.with_suffix('.onnx')\n",
    "ir_path = onnx_path.with_suffix(\".xml\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model\n",
    "\n",
    "Generally, PyTorch models represent an instance of the [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class, initialized by a state dictionary with model weights.\n",
    "We will load a TSM model pre-trained on the Kinetics-400 dataset. The TSM uses ResNet-50 as its 2D-CNN backbone. The [repo](https://github.com/mit-han-lab/temporal-shift-module) contains a list of TSM model weights trained on various temporal datasets with different model backbones.\n",
    "\n",
    "Typical steps to obtain pre-trained model:\n",
    "1. Download the pre-trained weights\n",
    "2. Create instance of model class\n",
    "3. Load checkpoint state dict, which contains pre-trained model weights\n",
    "4. Turn model to evaluation for switching some operations to inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download pre-trained model weights\n",
    "MODEL_LINK = f\"https://hanlab.mit.edu/projects/tsm/models/{weights_filename}\" \n",
    "print(f'Downloading TSM pretrained weights from URL: {MODEL_LINK}')\n",
    "download_file(MODEL_LINK, directory=MODEL_DIR, show_progress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a TSM model with the following arguments:\n",
    "* `num_class` - Number of classes (or labels) in the dataset\n",
    "* `num_segments` - Temporal size of the input image sequence\n",
    "* `modality` - Type of input image used (e.g. *RGB*, *Flow*, *RGBDiff*)\n",
    "* `base_model` - 2D-CNN model into which the TSM will be injected\n",
    "* `is_shift` - Apply temporal shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Initializing TSN with base model: resnet50.\n",
      "    TSN Configurations:\n",
      "        input_modality:     RGB\n",
      "        num_segments:       8\n",
      "        new_length:         1\n",
      "        consensus_module:   avg\n",
      "        dropout_ratio:      0.8\n",
      "        img_feature_dim:    256\n",
      "            \n",
      "=> base model: resnet50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Code\\Internships\\OpenVINO\\openvino_notebooks\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "c:\\Code\\Internships\\OpenVINO\\openvino_notebooks\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding temporal shift...\n",
      "=> n_segment per stage: [8, 8, 8, 8]\n",
      "=> Processing stage with 3 blocks residual\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Processing stage with 4 blocks residual\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Processing stage with 6 blocks residual\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Processing stage with 3 blocks residual\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n"
     ]
    }
   ],
   "source": [
    "from ops.models import TSN\n",
    "\n",
    "# Input settings\n",
    "SEGMENT_SIZE = 8\n",
    "IMAGE_WIDTH = 224\n",
    "IMAGE_HEIGHT = 224\n",
    "CHANNELS = 3\n",
    "MODALITY = 'RGB'\n",
    "\n",
    "# Step 2: Create instance of TSM model\n",
    "model = TSN(\n",
    "    num_class=400,\n",
    "    num_segments=SEGMENT_SIZE,\n",
    "    modality=MODALITY,\n",
    "    base_model='resnet50',    \n",
    "    is_shift=True\n",
    ")\n",
    "\n",
    "# Step 3: Load checkpoint state dict\n",
    "checkpoint = torch.load(weights_path, map_location='cpu')['state_dict']\n",
    "\n",
    "# Remove prefix 'module.' from model structure names\n",
    "base_dict = {k.replace('module.', '', 1): v for k, v in checkpoint.items()}\n",
    "model.load_state_dict(base_dict)\n",
    "\n",
    "# Step 4: Set the model to inference mode\n",
    "model.eval()\n",
    "\n",
    "# Record some model attributes that will be used later\n",
    "model_attr = {\n",
    "    'scale_size': model.scale_size,\n",
    "    'input_size': model.input_size,\n",
    "    'input_mean': model.input_mean,\n",
    "    'input_std': model.input_std\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX and OpenVINO IR Model Conversion\n",
    "\n",
    "### Convert PyTorch model to ONNX\n",
    "\n",
    "OpenVINO supports PyTorch models that are exported in ONNX format. We will use the `torch.onnx.export` function to obtain the ONNX model, you can learn more about this feature in the [PyTorch documentation](https://pytorch.org/docs/stable/onnx.html). We need to provide a model object, example input for model tracing and path where the model will be saved. When providing example input, it is not necessary to use real data, dummy input data with specified shape is sufficient. Optionally, we can provide a target onnx opset for conversion and/or other parameters specified in documentation (e.g. input and output names or dynamic shapes).\n",
    "\n",
    "Sometimes a warning will be shown, but in most cases it is harmless, so let us just filter it out. When the conversion is successful, the last line of the output will read: `ONNX model exported to ..\\model\\TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e50.onnx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model exported to ..\\model\\TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e50.onnx.\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    if not onnx_path.exists():\n",
    "        dummy_input = torch.randn(1, SEGMENT_SIZE, CHANNELS, IMAGE_HEIGHT, IMAGE_WIDTH)                \n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "            opset_version=11,           # the ONNX version to export the model to\n",
    "            input_names=['input'],      # the model's input names\n",
    "            output_names=['output'],    # the model's output names\n",
    "            dynamic_axes={\n",
    "                'input': {0: 'batch_size'},  # variable length axes\n",
    "                'output': {0: 'batch_size'}\n",
    "            }\n",
    "        )\n",
    "        print(f\"ONNX model exported to {onnx_path}.\")\n",
    "    else:\n",
    "        print(f\"ONNX model {onnx_path} already exists.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert ONNX Model to OpenVINO Intermediate Representation (IR)\n",
    "\n",
    "While ONNX models are directly supported by OpenVINO runtime, it can be useful to convert them to IR format to take the advantage of OpenVINO optimization tools and features. The `mo.convert_model` python function in OpenVINO Model Optimizer can be used for converting the model. The function returns instance of OpenVINO Model class, which is ready to use in Python interface. However, it can also be serialized to OpenVINO IR format for future execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.tools import mo\n",
    "from openvino.runtime import serialize\n",
    "\n",
    "model_ir = mo.convert_model(onnx_path)\n",
    "\n",
    "# Save IR model for future use\n",
    "serialize(model_ir, str(ir_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify model accuracy\n",
    "\n",
    "To confirm the successful conversion of the models, we will compare the accuracy of the converted models with that of the PyTorch model. We will evaluate the models on a subset of the [Kinetics-400](https://www.deepmind.com/open-source/kinetics) dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and prepare dataset\n",
    "\n",
    "The [Kinetics-400](https://www.deepmind.com/open-source/kinetics) is a human action video dataset that contains 400 human action classes, such as *making tea*, *shaking hands*, and *playing saxophone*. The videos are taken from YouTube and have a variable resolution and frame rate.\n",
    "The dataset is split into 3 partitions: *train*, *validate*, and *test*. We will use the *validate* split for the evaluation task.\n",
    "\n",
    "The *validate* split consists of 17,727 video records. For demonstration purposes, we will use a very small portion of this split.\n",
    "\n",
    "We will follow the steps below for preparing the dataset for evaluation:\n",
    "1. Download the [dataset files](https://storage.googleapis.com/deepmind-media/Datasets/kinetics400.tar.gz) which contain the list of videos for each partition and the meta-data for each video.\n",
    "2. Download a limited number of YouTube videos listed in the *validate* partition file.\n",
    "3. Convert each video (or a portion thereof) to image frames, and save these in folders corresponding to their class labels.\n",
    "4. Generate mappings between each video sequence and class indices.\n",
    "\n",
    "The code below is adapted from *vid2img_kinetics.py* and *gen_label_kinetics.py* files in the TSM [repo](https://github.com/mit-han-lab/temporal-shift-module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "DATA_URL = 'https://storage.googleapis.com/deepmind-media/Datasets/kinetics400.tar.gz'\n",
    "\n",
    "# Download dataset partition information\n",
    "download_file(DATA_URL, directory=DATA_ROOT_DIR, show_progress=True)\n",
    "\n",
    "with tarfile.open(DATA_ROOT_DIR / 'kinetics400.tar.gz', \"r\") as tar_ref:\n",
    "    tar_ref.extractall(DATA_ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from csv import DictReader\n",
    "from pytube import YouTube\n",
    "from pytube.exceptions import PytubeError\n",
    "\n",
    "\n",
    "def sanitise_class_labels(label: str) -> str:\n",
    "    \"\"\" \n",
    "    Filter out unwanted characters from class names/labels.\n",
    "    \n",
    "    :param label: class label\n",
    "    :returns: sanitised class label  \n",
    "    \"\"\"\n",
    "    label = re.sub(' ', '_', label)\n",
    "    label = re.sub(r\"[()'\\\"]\", '', label)\n",
    "    return label\n",
    "\n",
    "\n",
    "def download_youtube_video(video_id: str, filename: PathLike, directory: PathLike):\n",
    "    \"\"\"\n",
    "    Download a YouTube video to a given file path\n",
    "\n",
    "    :param video_id: ID of YouTube video\n",
    "    :param filename: Name of the local file to save\n",
    "    :param directory: Directory to save the file to\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    directory = Path(directory)\n",
    "    if not directory.exists():\n",
    "        directory.mkdir(parents=True)\n",
    "\n",
    "    yt = YouTube.from_id(video_id)\n",
    "\n",
    "    try:        \n",
    "        streams = yt.streams.filter(file_extension='mp4').get_highest_resolution()\n",
    "        streams.download(output_path=directory, filename=filename)        \n",
    "    except PytubeError as err:\n",
    "        raise Exception(f\"Downloading of video {video_id} failed with error: {err}\") from None\n",
    "\n",
    "\n",
    "def prepare_dataset(video_list_file: Path, data_dir: Path, max_videos: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Download videos listed in file and convert them to image frames.\n",
    "    \n",
    "    :param video_list_file: Full path to file that contains information of videos \n",
    "                            to be downloaded. Each line should contain the following:\n",
    "                            [label,youtube_id,time_start,time_end,dataset_split]\n",
    "    :param data_dir: Root directory to save videos and extracted image frames to\n",
    "    :param max_videos: If provided, the maximum number of videos to download\n",
    "    \"\"\"\n",
    "\n",
    "    # Create output directories\n",
    "    video_root_dir = data_dir / 'videos/'\n",
    "    imgs_root_dir = data_dir / 'images/'\n",
    "    \n",
    "    with open(video_list_file, 'r') as f:\n",
    "        # Read in the list of videos to download and additional information\n",
    "        dict_reader = DictReader(f)\n",
    "        video_list = list(dict_reader)\n",
    "        print(f'Found {len(video_list)} video records in file \"{video_list_file.name}\"')\n",
    "        \n",
    "        print(f'\\nProcessing videos:')\n",
    "        for i, info in enumerate(video_list):\n",
    "            if max_videos is not None and i >= max_videos:\n",
    "                break\n",
    "            \n",
    "            youtube_id = info['youtube_id']\n",
    "\n",
    "            # Extract and sanitise class name\n",
    "            class_name = sanitise_class_labels(info['label'])           \n",
    "\n",
    "            # Save video to the associated class directory\n",
    "            video_path = video_root_dir / class_name / f\"v{youtube_id}.mp4\"\n",
    "            if not video_path.parent.exists():\n",
    "                video_path.parent.mkdir(parents=True)\n",
    "\n",
    "            # Download YouTube video\n",
    "            print(f\"\\t[{class_name}] Downloading '{video_path.name}' ... \", end='')\n",
    "            try:                \n",
    "                download_youtube_video(\n",
    "                    video_id=youtube_id,\n",
    "                    filename=video_path.name,\n",
    "                    directory=video_path.parent\n",
    "                )\n",
    "                print('downloaded')\n",
    "            except Exception as err:\n",
    "                print(f\"\\n\\t[ERROR]: {err}\")\n",
    "                continue\n",
    "            \n",
    "            # Extract image frames from video segment\n",
    "            seg_start = int(info['time_start'])\n",
    "            seg_end = int(info['time_end'])\n",
    "\n",
    "            # Create images directory with video name\n",
    "            imgs_out_dir = imgs_root_dir / class_name / video_path.stem\n",
    "            if imgs_out_dir.exists() and len(list(imgs_out_dir.glob('*.jpg'))) > 0:\n",
    "                print('\\t** Conversion already done **\\n')\n",
    "                continue\n",
    "            else:\n",
    "                imgs_out_dir.mkdir(parents=True)\n",
    "            \n",
    "            print(f'\\tExtracting video frames (segment: {seg_start} - {seg_end} seconds)\\n')\n",
    "            extract_cmd = f'ffmpeg -ss {seg_start} -i \"{str(video_path)}\" ' \\\n",
    "                            f'-to {seg_end} -loglevel error ' \\\n",
    "                            f'-threads 1 -vf scale=-1:331 ' \\\n",
    "                            f'-q:v 0 \"{str(imgs_out_dir)}/img_%05d.jpg\"'\n",
    "            ! $extract_cmd            \n",
    "\n",
    "    print('Dataset preparation complete.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We limit the number of videos to download to **10**. Feel free to adjust the `max_videos` value or set it to `None` if you wish to download the entire split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17727 video records in file \"validate.csv\"\n",
      "\n",
      "Processing videos:\n",
      "\t[javelin_throw] Downloading 'v--07WQ2iBlw.mp4' ... downloaded\n",
      "\t** Conversion already done **\n",
      "\n",
      "\t[flipping_pancake] Downloading 'v--33Lscn6sk.mp4' ... downloaded\n",
      "\t** Conversion already done **\n",
      "\n",
      "Dataset preparation complete.\n"
     ]
    }
   ],
   "source": [
    "prepare_dataset(\n",
    "    video_list_file=DATA_DIR / 'validate.csv',\n",
    "    data_dir=DATA_DIR,\n",
    "    max_videos=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_video_class_mappings(labels_file: PathLike, imgs_root_dir: PathLike, out_file: PathLike):\n",
    "    \"\"\" \n",
    "    Generate a file containing image paths mapped to corresponding class indices and frame counts.\n",
    "\n",
    "    :param labels_file: Path to file that contains a list of all dataset labels\n",
    "    :param imgs_root_dir: Directory containing extracted video image frames\n",
    "    :param out_file: Path to output file\n",
    "    \"\"\"\n",
    "\n",
    "    # Read Kitenics-400 class list from file\n",
    "    with open(labels_file) as f:\n",
    "        classes = f.readlines()\n",
    "        classes = [sanitise_class_labels(c.strip()) for c in classes]\n",
    "        print(f'{len(classes)} classes found')\n",
    "\n",
    "    # Map classes to numeric indices\n",
    "    class_mapping = {c: i for i, c in enumerate(classes)}\n",
    "\n",
    "    # Loop through image frame folders and map each video to a class index\n",
    "    imgs_root_dir = Path(imgs_root_dir)\n",
    "\n",
    "    output = []\n",
    "\n",
    "    for class_dir in imgs_root_dir.iterdir():\n",
    "        class_name = class_dir.name\n",
    "        class_index = class_mapping[class_name]\n",
    "\n",
    "        for imgs_dir in class_dir.iterdir():\n",
    "            if imgs_dir.is_dir():\n",
    "                # Count the number of frames in each video folder\n",
    "                num_frames = len(list(imgs_dir.iterdir()))\n",
    "                img_rel_path = '/'.join(imgs_dir.parts[-2:])\n",
    "                info = f'{img_rel_path} {num_frames} {class_index}'\n",
    "                output.append(info)\n",
    "\n",
    "    if len(output) > 0:\n",
    "        with open(out_file, 'w') as f:\n",
    "            f.write('\\n'.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 classes found\n"
     ]
    }
   ],
   "source": [
    "# File containing all 400 labels of the Kinetics-400 dataset\n",
    "labels_file = './tools/kinetics_label_map.txt'\n",
    "# The full path to the output file\n",
    "video_mapping_file = DATA_DIR / 'labels/video_class_mappings.txt'\n",
    "\n",
    "# Create parent directory for output file\n",
    "Path(video_mapping_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "generate_video_class_mappings(labels_file, IMAGES_DIR, video_mapping_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video number:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Code\\Internships\\OpenVINO\\openvino_notebooks\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from ops.dataset import TSNDataSet\n",
    "from ops.transforms import *\n",
    "\n",
    "# Initialise model transforms\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    GroupScale(model_attr['scale_size']),\n",
    "    GroupCenterCrop(model_attr['input_size']),\n",
    "    Stack(roll=False), \n",
    "    ToTorchFormatTensor(div=True),\n",
    "    GroupNormalize(model_attr['input_mean'], model_attr['input_std']),\n",
    "])\n",
    "\n",
    "# Create dataset and data loader\n",
    "dataset = TSNDataSet(\n",
    "    root_path=IMAGES_DIR,\n",
    "    list_file=video_mapping_file,\n",
    "    num_segments=SEGMENT_SIZE,\n",
    "    new_length=1,\n",
    "    modality=MODALITY,\n",
    "    test_mode=True,\n",
    "    remove_missing=True,\n",
    "    transform=transforms\n",
    ")\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=32, pin_memory=True, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define validation functions\n",
    "\n",
    "We will adopt the evaluation metrics used in the TSM [paper](https://arxiv.org/abs/1811.08383), namely, the Top-1 and Top-5 accuracy scores. We will use scikit-learn's [top_k_accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.top_k_accuracy_score.html) function which computes the number of times the correct label is in the top `k` predictions (ranked by predicted scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs(images: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Expand the input image sequence shape to include a temporal dimension.\n",
    "        \n",
    "    :param images: Image sequence of shape [N,T*C,H,W]\n",
    "    :returns: Image sequence reshaped to [N,T,C,H,W]\n",
    "    \"\"\"\n",
    "    \n",
    "    [batch_size, tc, height, width] = images.size()\n",
    "\n",
    "    # Decompose the second dimension into channels and number of segments (temporal size)\n",
    "    channels = 3  # RGB\n",
    "    n_segments = int(tc / channels)\n",
    "    assert channels * n_segments == tc\n",
    "\n",
    "    images = images.view(batch_size, n_segments, channels, height, width)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino.runtime as ov\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "def validate(model: Union[ov.CompiledModel, torch.nn.Module],\n",
    "             validation_loader: DataLoader,\n",
    "             top_k: Tuple[int , ...] = (1,)) -> Dict[int, float]:\n",
    "    \"\"\" \n",
    "    Evaluate TSM model and compute accuracy metrics.\n",
    "\n",
    "    :param model: Model to validate\n",
    "    :param validation_loader: Validation dataset\n",
    "    :param top_k: Number of top elements to look at for computing accuracy. Allows\n",
    "                  for the computation of multiple top_k metrics.\n",
    "    :returns: Accuracy scores for all `k`s provided\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    is_compiled = isinstance(model, ov.CompiledModel)\n",
    "    output = model.outputs[0] if is_compiled else None\n",
    "  \n",
    "    for images, target in validation_loader:\n",
    "        images = prepare_inputs(images)\n",
    "\n",
    "        if is_compiled:\n",
    "            pred = model(images)[output]\n",
    "        else:\n",
    "            pred = model(images)\n",
    "            pred = pred.detach().numpy()\n",
    "\n",
    "        predictions.append(pred)\n",
    "        references.append(target)\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    references = np.concatenate(references, axis=0)\n",
    "\n",
    "    # Generate a list of all class \"labels\" (i.e. indices)\n",
    "    class_indices = list(range(predictions.shape[-1]))\n",
    "\n",
    "    scores = {}\n",
    "    for k in top_k:\n",
    "        scores[k] = top_k_accuracy_score(references, predictions, k=k, labels=class_indices)        \n",
    "    return scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate PyTorch, ONNX, and IR models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "\n",
    "core = Core()\n",
    "\n",
    "# Read converted models\n",
    "model_ir = core.read_model(ir_path)\n",
    "model_onnx = core.read_model(onnx_path)\n",
    "\n",
    "# Compile models on CPU device\n",
    "compiled_model_ir = core.compile_model(model_ir, 'CPU')\n",
    "compiled_model_onnx = core.compile_model(model_onnx, 'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy Results\n",
      "------------------------------------------------------------\n",
      "OpenVINO IR Model  |  Prec@1:  66.67 %  |  Prec@5: 100.00 %\n",
      "ONNX Model         |  Prec@1:  66.67 %  |  Prec@5: 100.00 %\n",
      "PyTorch Model      |  Prec@1:  66.67 %  |  Prec@5: 100.00 %\n"
     ]
    }
   ],
   "source": [
    "val_models = {\n",
    "    'PyTorch': model,\n",
    "    'ONNX': compiled_model_onnx,\n",
    "    'OpenVINO IR': compiled_model_ir\n",
    "}\n",
    "\n",
    "print(f'Model Accuracy Comparison\\n{\"-\" * 60}')\n",
    "for name, val_model in val_models.items():\n",
    "    scores = validate(val_model, data_loader, top_k=(1, 5))\n",
    "\n",
    "    # Print results\n",
    "    print(f'{name + \" Model\":17s}', end='')\n",
    "    for k, score in scores.items():\n",
    "        print(f'  |  Prec@{k}: {score * 100:-6.2f} %', end='')\n",
    "    print('')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize model using NNCF Post-training Quantization API\n",
    "\n",
    "The OpenVINOâ„¢ [Neural Network Compression Framework](https://docs.openvino.ai/latest/nncf_ptq_introduction.html) (NNCF) provides a suite of advanced algorithms for Neural Networks inference optimization with minimal accuracy drop.\n",
    "\n",
    "We will apply the post-training 8-bit integer quantization method which converts weights and activations from floating-point precision to integer precision, thus reducing the model size, memory footprint, and latency, as well as improving the computational efficiency using integer arithmetic.\n",
    "\n",
    "The optimization process contains the following steps:\n",
    "* Prepare the calibration dataset that is used to estimate quantization parameters of the activations within the model.\n",
    "* Call `nncf.quantize` to apply 8-bit quantization to the model.\n",
    "* Save the quantized model using `openvino.runtime.serialize`.\n",
    "\n",
    "We will re-use the validation dataloader for the quantization process. This is achieved by wrapping the dataloader into the `nncf.Dataset` object and defining transformation function which extracts the input data and returns it in the state required by the model.\n",
    "\n",
    "Note that the quantization process may take quite some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_fn(data_item):\n",
    "    \"\"\"\n",
    "    Quantization transform function. Extracts and preprocesses input data from dataloader item\n",
    "    for quantization.\n",
    "\n",
    "    :param data_item: Tuple with data item produced by DataLoader during iteration\n",
    "    :returns: Input data for quantization\n",
    "    \"\"\"\n",
    "    images, _ = data_item\n",
    "    return prepare_inputs(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openvino.tools.pot.pipeline.pipeline:Inference Engine version:                2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "INFO:openvino.tools.pot.pipeline.pipeline:Model Optimizer version:                 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "INFO:openvino.tools.pot.pipeline.pipeline:Post-Training Optimization Tool version: 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "INFO:openvino.tools.pot.statistics.collector:Start computing statistics for algorithms : DefaultQuantization\n",
      "INFO:openvino.tools.pot.statistics.collector:Computing statistics finished\n",
      "INFO:openvino.tools.pot.pipeline.pipeline:Start algorithm: DefaultQuantization\n",
      "INFO:openvino.tools.pot.algorithms.quantization.default.algorithm:Start computing statistics for algorithm : ActivationChannelAlignment\n",
      "INFO:openvino.tools.pot.algorithms.quantization.default.algorithm:Computing statistics finished\n",
      "INFO:openvino.tools.pot.algorithms.quantization.default.algorithm:Start computing statistics for algorithms : MinMaxQuantization,FastBiasCorrection\n",
      "INFO:openvino.tools.pot.algorithms.quantization.default.algorithm:Computing statistics finished\n",
      "INFO:openvino.tools.pot.pipeline.pipeline:Finished: DefaultQuantization\n",
      " ===========================================================================\n"
     ]
    }
   ],
   "source": [
    "import nncf\n",
    "\n",
    "calibration_dataset = nncf.Dataset(data_loader, transform_fn)\n",
    "print('Calibration dataset created.')\n",
    "\n",
    "quantized_model = nncf.quantize(model_ir, calibration_dataset, subset_size=1, preset=nncf.QuantizationPreset.MIXED)\n",
    "print('Model successfully quantized.')\n",
    "\n",
    "# Create filename by appending '_int8' to IR model filename \n",
    "quant_path = ir_path.parent / f'{ir_path.stem}_int8{ir_path.suffix}'\n",
    "\n",
    "# Save quantized model\n",
    "serialize(quantized_model, str(quant_path))\n",
    "print(f'Quantized model saved to {quant_path}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate quantized model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Model Accuracy Results:\n",
      "\tPrec@1:  66.67 %\n",
      "\tPrec@5: 100.00 %\n"
     ]
    }
   ],
   "source": [
    "int8_compiled_model = core.compile_model(quantized_model)\n",
    "\n",
    "int8_scores = validate(int8_compiled_model, data_loader, top_k=(1, 5))\n",
    "\n",
    "# Print results\n",
    "print('Quantized Model Accuracy Results:')\n",
    "for k, score in int8_scores.items():\n",
    "    print(f'\\tPrec@{k}: {score * 100:-6.2f} %')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Performance of the Original and Quantized Models\n",
    "Finally, use the OpenVINO [Benchmark Tool](https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html) to measure the inference performance of the `FP32` and `INT8` models.\n",
    "\n",
    "> **NOTE**: For more accurate performance, it is recommended to run `benchmark_app` in a terminal/command prompt after closing other applications. Run `benchmark_app -m model.xml -d CPU` to benchmark async inference on CPU for one minute. Change `CPU` to `GPU` to benchmark on GPU. Run `benchmark_app --help` to see an overview of all command-line options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape: \"[1, 8, 3, 224, 224]\"\n",
      "!benchmark_app -m ..\\model\\TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e50.xml -shape \"[1, 8, 3, 224, 224]\" -d CPU -api async\n",
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to THROUGHPUT.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 1204.65 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input (node: input) : f32 / [...] / [?,8,3,224,224]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     output (node: output) : f32 / [...] / [?,400]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[ INFO ] Reshaping model: 'input': [1,8,3,224,224]\n",
      "[ INFO ] Reshape model took 350.55 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input (node: input) : f32 / [...] / [1,8,3,224,224]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     output (node: output) : f32 / [...] / [1,400]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 1344.03 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch_jit\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 4\n",
      "[ INFO ]   NUM_STREAMS: 4\n",
      "[ INFO ]   AFFINITY: Affinity.NONE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 8\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.THROUGHPUT\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 4 inference requests, limits: 60000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 1155.79 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Count:            72 iterations\n",
      "[ INFO ] Duration:         72205.94 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        3475.61 ms\n",
      "[ INFO ]    Average:       3990.01 ms\n",
      "[ INFO ]    Min:           2900.24 ms\n",
      "[ INFO ]    Max:           6751.71 ms\n",
      "[ INFO ] Throughput:   1.00 FPS\n",
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to THROUGHPUT.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 4586.13 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input (node: input) : f32 / [...] / [?,8,3,224,224]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     output (node: output) : f32 / [...] / [?,400]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[ INFO ] Reshaping model: 'input': [1,8,3,224,224]\n",
      "[ INFO ] Reshape model took 1020.11 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input (node: input) : f32 / [...] / [1,8,3,224,224]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     output (node: output) : f32 / [...] / [1,400]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 2015.12 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch_jit\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 4\n",
      "[ INFO ]   NUM_STREAMS: 4\n",
      "[ INFO ]   AFFINITY: Affinity.NONE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 8\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.THROUGHPUT\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 4 inference requests, limits: 60000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 1040.35 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Count:            48 iterations\n",
      "[ INFO ] Duration:         67077.18 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        5650.48 ms\n",
      "[ INFO ]    Average:       5513.68 ms\n",
      "[ INFO ]    Min:           3527.63 ms\n",
      "[ INFO ]    Max:           7488.33 ms\n",
      "[ INFO ] Throughput:   0.72 FPS\n"
     ]
    }
   ],
   "source": [
    "# Inference FP32 model (OpenVINO IR)\n",
    "input_shape = f'\"{[1, SEGMENT_SIZE, CHANNELS, IMAGE_HEIGHT, IMAGE_WIDTH]}\"'\n",
    "print(f\"!benchmark_app -m {ir_path} -shape {input_shape} -d CPU -api async\")\n",
    "\n",
    "!benchmark_app -m {ir_path} -shape {input_shape} -d CPU -api async\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!benchmark_app -m ..\\model\\TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e50_int8.xml -shape \"[1, 8, 3, 224, 224]\" -d CPU -api async\n",
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to THROUGHPUT.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 1259.10 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input (node: input) : f32 / [...] / [?,8,3,224,224]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     output (node: output) : f32 / [...] / [?,400]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[ INFO ] Reshaping model: 'input': [1,8,3,224,224]\n",
      "[ INFO ] Reshape model took 371.04 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input (node: input) : f32 / [...] / [1,8,3,224,224]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     output (node: output) : f32 / [...] / [1,400]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 2268.91 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch_jit\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 4\n",
      "[ INFO ]   NUM_STREAMS: 4\n",
      "[ INFO ]   AFFINITY: Affinity.NONE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 8\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.THROUGHPUT\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 4 inference requests, limits: 60000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 717.36 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Count:            88 iterations\n",
      "[ INFO ] Duration:         65339.22 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        2921.93 ms\n",
      "[ INFO ]    Average:       2920.91 ms\n",
      "[ INFO ]    Min:           1768.59 ms\n",
      "[ INFO ]    Max:           6157.14 ms\n",
      "[ INFO ] Throughput:   1.35 FPS\n",
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to THROUGHPUT.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 6393.10 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input (node: input) : f32 / [...] / [?,8,3,224,224]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     output (node: output) : f32 / [...] / [?,400]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[ INFO ] Reshaping model: 'input': [1,8,3,224,224]\n",
      "[ INFO ] Reshape model took 2112.27 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input (node: input) : f32 / [...] / [1,8,3,224,224]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     output (node: output) : f32 / [...] / [1,400]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 10781.03 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch_jit\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 4\n",
      "[ INFO ]   NUM_STREAMS: 4\n",
      "[ INFO ]   AFFINITY: Affinity.NONE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 8\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.THROUGHPUT\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 4 inference requests, limits: 60000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 3814.71 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Count:            84 iterations\n",
      "[ INFO ] Duration:         63451.62 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        2703.40 ms\n",
      "[ INFO ]    Average:       2978.68 ms\n",
      "[ INFO ]    Min:           2233.83 ms\n",
      "[ INFO ]    Max:           6127.32 ms\n",
      "[ INFO ] Throughput:   1.32 FPS\n"
     ]
    }
   ],
   "source": [
    "# Inference INT8 model (OpenVINO IR)\n",
    "print(f\"!benchmark_app -m {quant_path} -shape {input_shape} -d CPU -api async\")\n",
    "!benchmark_app -m {quant_path} -shape {input_shape} -d CPU -api async"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea35bed4eb71ed03c6b6dc92016a0ad5975e301e1c045d6e243bf5a778dd1846"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
